{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Tree Search Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, current_process\n",
    "\n",
    "from board import Board\n",
    "from player import Player\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me    = 'o'\n",
    "agent = 'x'\n",
    "nrow  = 5\n",
    "ncol  = 5\n",
    "\n",
    "board  = Board(nrow=nrow, ncol=ncol, sign_play=[agent,me])\n",
    "player = Player(sign=agent, order=float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, N, Q = player.choose_action_mcts(board, num_sim=10**2, return_dicts=True)\n",
    "\n",
    "processes = []\n",
    "\n",
    "for \n",
    "    process = Process(target=player.choose_action_mcts, args=(board, num_sim=10**2, return_dicts=True))\n",
    "    process.append(process)\n",
    "    \n",
    "    # processes are spawned\n",
    "    process.start()\n",
    "    \n",
    "for process in processes:\n",
    "    # wait for each process to terminate before contunuing with the code\n",
    "    process.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "c = 4\n",
    "\n",
    "st = f\"{a} is {b} + {c}\"\n",
    "\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Process, Manager\n",
    "import multiprocessing as mp\n",
    "import itertools as itr\n",
    "from utils import *\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from tqdm import trange\n",
    "\n",
    "from board import Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/6832554/multiprocessing-how-do-i-share-a-dict-among-multiple-processes\n",
    "\n",
    "def choose_action_mcts_mp(board, num_sim=10**2, return_dicts=False):\n",
    "    \"\"\"\n",
    "    explain Monte Carlo Tree Search\n",
    "    \"\"\"\n",
    "    manager = mp.Manager()\n",
    "    N = manager.dict()      # visit count\n",
    "    Q = manager.dict()      # mean action value\n",
    "    P = manager.dict()      # prior probability of that action, {} or could load existing dictionary\n",
    "    c = 1                   # exploration/exploitation trade-off\n",
    "\n",
    "    p = mp.Pool(processes=mp.cpu_count())\n",
    "    p.starmap(mcts_simulation, itr.repeat((board, N, Q, P, c), times=num_sim))\n",
    "    p.close()\n",
    "    p.join()\n",
    "\n",
    "#     # perform the simulations\n",
    "#     for n in trange(num_sim):\n",
    "#         N, Q = self.mcts_simulation(board, N, Q, P, c)\n",
    "\n",
    "    # next possible states\n",
    "    next_states = board.get_next_states(sign='x')    \n",
    "    # get count for each next state\n",
    "    next_counts = [N.get(state, 0) for state in next_states]\n",
    "    # randomly select action according to weights in next_counts\n",
    "    action = random.choices(board.get_free_positions(), weights=normalize(next_counts, float('inf')))[0]\n",
    "\n",
    "    return action, N, Q if return_dicts else action\n",
    "\n",
    "\n",
    "def choose_action_mcts(board, num_sim=10**2, return_dicts=False):\n",
    "    \"\"\"\n",
    "    explain Monte Carlo Tree Search\n",
    "    \"\"\"\n",
    "\n",
    "    N = {}      # visit count\n",
    "    Q = {}      # mean action value\n",
    "    P = {}      # prior probability of that action, {} or could load existing dictionary\n",
    "    c = 1       # exploration/exploitation trade-off\n",
    "    \n",
    "    # perform the simulations\n",
    "    for n in range(num_sim):\n",
    "        mcts_simulation(board, N, Q, P, c)\n",
    "\n",
    "    # next possible states\n",
    "    next_states = board.get_next_states(sign='x')    \n",
    "    # get count for each next state\n",
    "    next_counts = [N.get(state, 0) for state in next_states]\n",
    "    # randomly select action according to weights in next_counts\n",
    "    action = random.choices(board.get_free_positions(), weights=normalize(next_counts, float('inf')))[0]\n",
    "\n",
    "    return action, N, Q if return_dicts else action\n",
    "\n",
    "\n",
    "def mcts_simulation(board, N, Q, P, c):\n",
    "    \"\"\"\n",
    "    explain: select, expand and evaluate, backup\n",
    "    \"\"\"\n",
    "    # play on a copy of the board\n",
    "    board_cpy = copy.deepcopy(board)\n",
    "    # store all the states of this MCTS simulation\n",
    "    board_states = []\n",
    "\n",
    "    # assume that the game will be a draw\n",
    "    reward = 0.5\n",
    "    while not board_cpy.is_full():\n",
    "\n",
    "        # update visit count (necessary because of self-play = inverse board)\n",
    "        N[board_cpy.get_state()] = N.get(board_cpy.get_state(), 0) + 1\n",
    "\n",
    "        # evaluate possible actions\n",
    "        next_states = board_cpy.get_next_states(sign='x')\n",
    "        ucb_states  = []\n",
    "        for state in next_states:\n",
    "            q  = Q.get(state, 0.5)\n",
    "            p  = P.get(state, 1/len(next_states))\n",
    "            na = N.get(state, 0)\n",
    "            nb = N.get(board_cpy.get_state())\n",
    "            ucb_states.append(q + c * p * math.sqrt(nb) / (1+na))\n",
    "\n",
    "        # select action that maximizes the UCB value\n",
    "        action = random.choices(board_cpy.get_free_positions(), weights=normalize(ucb_states, float('inf')))[0]\n",
    "        # take action\n",
    "        board_cpy.add(sign='x', row=action[0], col=action[1])\n",
    "        # update visit count\n",
    "        N[board_cpy.get_state()] = N.get(board_cpy.get_state(), 0) + 1\n",
    "        # add board state to list of visited states\n",
    "        board_states.append(board_cpy.get_state())\n",
    "\n",
    "        # check if player won\n",
    "        if board_cpy.is_won(): \n",
    "            reward = 1\n",
    "            break\n",
    "        # if nobody won yet, inverse the board\n",
    "        board_cpy.inverse()   \n",
    "\n",
    "    # backup\n",
    "    board_states.reverse()\n",
    "    # update each board value\n",
    "    for state in board_states:\n",
    "        q = Q.get(state, 0.5)\n",
    "        n = N.get(state)\n",
    "        # incremental mean formula\n",
    "        Q[state] = q + (reward - q) / n\n",
    "        # inverse reward due to self-play\n",
    "        reward = 1-reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 98/100000 [00:00<03:19, 501.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-processing took 319.64283561706543 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [04:31<00:00, 368.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no multi-processing took 271.7143943309784 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "me    = 'o'\n",
    "agent = 'x'\n",
    "nrow  = 3\n",
    "ncol  = 3\n",
    "\n",
    "board  = Board(nrow=nrow, ncol=ncol, sign_play=[agent,me])\n",
    "board.set_state('xox-ox--o')\n",
    "# board.print()\n",
    "\n",
    "start = time.time()\n",
    "action, N, Q = choose_action_mcts_mp(board, num_sim=10**5, return_dicts=True)\n",
    "end = time.time()\n",
    "print(f\"multi-processing took {end-start} seconds\")\n",
    "\n",
    "start = time.time()\n",
    "action, N, Q = choose_action_mcts(board, num_sim=10**5, return_dicts=True)\n",
    "end = time.time()\n",
    "print(f\"no multi-processing took {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N.get('xox-ox--o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### once mp works for mcts, also apply on \"value_deeper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_deeper(board, agent, order, all_values):\n",
    "    \"\"\"\n",
    "    compute values to which training converges\n",
    "    \"\"\"\n",
    "    free_pos = board.get_free_positions()\n",
    "    next_vals = []\n",
    "    for pos in free_pos:\n",
    "        # add symbol on that new position\n",
    "        next_board = copy.deepcopy(board)\n",
    "        next_board.add(sign=agent, row=pos[0], col=pos[1])\n",
    "        next_board_state = next_board.get_state()\n",
    "        # if win\n",
    "        if next_board.is_won(): \n",
    "            val = 1\n",
    "        # if draw\n",
    "        elif next_board.is_full():\n",
    "            val = 0.5\n",
    "        # if game not done\n",
    "        else:\n",
    "            next_board.inverse()\n",
    "            val = 1 - value_deeper(next_board, agent, order, all_values)\n",
    "\n",
    "        all_values[next_board_state] = val\n",
    "        next_vals.append(val)\n",
    "    # weigthed sum of al lnext values\n",
    "    weights = normalize(next_vals, order)\n",
    "    return np.dot(weights, next_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
